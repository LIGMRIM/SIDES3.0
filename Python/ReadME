1) convert file to ascii using linux command 'konwert utf8-ascii input_file.html -o output_file.csv' (no matter which extension it has, as long as it is a text file)
2) run script cleanHTMLandLemmatize.py
3) run script clean_french_stop_words.py
4) export the data, one file per class
5) dump only the text from all the classes into one single text file and process it with Google's word2vec software from: https://code.google.com/archive/p/word2vec/
./word2phrase -train text_data.txt -output data.phrase0 -threshold 200 -debug 2 -min-count 5
./word2phrase -train data.phrase0 -output data.phrase1 -threshold 100 -debug 2
./word2vec -train data.phrase1 -output mr-LEMMAcbow0Size50Window5.bin -cbow 0 -size 50 -window 5 -negative 20 -hs 0 -sample 1e-5 -threads 20 -binary 1 -iter 30
6) build input to the Deep Learning software
python process_data-LIG.py mr-LEMMAcbow0Size50Window5.bin #generates mr.p file, concatenates size of biggest sentence to the file
7) execute python ./conv_net_sentence-LIG.py mr.p size_of_biggest_sentence -nonstatic -rand

#setting up theano
a)install miniconda for python 2.7 (doesn't work with python 3)
b)install theano and pygpu (conda install theano pygpu)
c)numpy downgrade to 1.12 (install install numpy = 1.12)
d) fix a theano installation bug as described in https://stackoverflow.com/questions/53423610/how-to-update-scan-cython-code-in-theano
